{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanan\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import textblob as textblob\n",
    "import sklearn\n",
    "import numpy as np\n",
    "# Data is to be loaded from the dataset link present at : https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "df = pd.read_table('./smsspamcollection/SMSSpamCollection', sep='\\t', header=None, names=['label', 'sms_message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                        sms_message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Output and printing out the above\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of messages that are present in the dataset: 5572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1                        Ok lar... Joking wif u oni...\n",
       "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    U dun say so early hor... U c already then say...\n",
       "4    Nah I don't think he goes to usf, he lives aro...\n",
       "5    FreeMsg Hey there darling it's been 3 week's n...\n",
       "6    Even my brother is not like to speak with me. ...\n",
       "7    As per your request 'Melle Melle (Oru Minnamin...\n",
       "8    WINNER!! As a valued network customer you have...\n",
       "9    Had your mobile 11 months or more? U R entitle...\n",
       "Name: sms_message, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets find out the number of messages present\n",
    "print(\"The number of messages that are present in the dataset:\",len(df['label']))\n",
    "df['sms_message'][1:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the train set : 4179\n",
      "Number of rows in the train set : 1393\n"
     ]
    }
   ],
   "source": [
    "# Using Bag of Words(BoW) concept and tokenzing the string into individual words that gives Integer ID to each token\n",
    "# sklearn count vectorizer is being used here\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Split the data into train and test test\n",
    "'''\n",
    "using sklearn to split the data into train and test set\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Map dataframe to encode values and put values into a numpy array\n",
    "encoded_labels = df['label'].map(lambda x: 1 if x == 'spam' else 0).values # ham will be 0 and spam will be 1\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['sms_message'],\n",
    "                                                   encoded_labels,\n",
    "                                                   random_state=3)\n",
    "\n",
    "\n",
    "# checking the number of rows for each of the sets above\n",
    "print(\"Number of rows in the train set : {}\".format(X_train.shape[0]))\n",
    "print(\"Number of rows in the train set : {}\".format(X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Bag of words processing to our dataset using CountVectorizer()\n",
    "\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "# Fit the training data and return the matrix\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "\n",
    "#Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()\n",
    "testing_data = count_vector.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Instructions: \n",
    "\n",
    "We have loaded the training data into the variable \"training_data\", and testing data into the variabke \"testing data\"\n",
    "\n",
    "we will use the MultinomialNB classifier or GaussianNB classifier to fit the training data into the classifier using fit()\n",
    "Name your classifier \n",
    "\n",
    "you will be  training the classifier using the training data and y_train \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Now that your algorithm has been trained using the training data, we can use it on the testing data to make predictions\n",
    "\n",
    "'''\n",
    "\n",
    "predictions = naive_bayes.predict(testing_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Predictions and writing your own functions to calculate precision score \n",
    "'''\n",
    "Precision score = Total Positive rate = (True Positives)/(True Positives + False Positives)\n",
    "Recall score = Sensitivity = (True positives)/(True Positives + False Negatives)\n",
    "\n",
    "True Positives -> Sum of all the 1's in encoded labels\n",
    "False Positives -> Sum of all the 1's in the prediction list which are actually no 1's in encoded labels\n",
    "'''\n",
    "\n",
    "\n",
    "# Using numpy logic functions rather than for loops\n",
    "comp1 = np.equal(predictions, y_test)\n",
    "\n",
    "#Using itertools as we would need to get the ones which are not-1 in y_test\n",
    "\n",
    "def find_precision_recall_score(predictions, testset):\n",
    "    count_false_neg = 0\n",
    "    count_false_pos = 0\n",
    "    count_true_pos = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    for y1, y2 in zip(predictions, y_test):\n",
    "        if y2 == 1:\n",
    "            count_true_pos += 1\n",
    "            if y1 != 1:\n",
    "                count_false_neg += 1\n",
    "        elif y2 !=1:\n",
    "            if y1 == 1:\n",
    "                count_false_pos += 1\n",
    "    precision = (count_true_pos/(count_true_pos+count_false_pos))\n",
    "    recall = (count_true_pos/(count_true_pos+count_false_neg))\n",
    "    return(precision, recall)            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9877961234745154\n",
      "Precision score:  0.9837837837837838\n",
      "f1 score:  0.9553805774278217\n",
      "recall score: 0.9285714285714286\n",
      "Precision, Recall (0.9849246231155779, 0.9333333333333333)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now that the predictions are made lets check the measures below :\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "'''\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "print('Accuracy score: ',  format(accuracy_score(y_test,predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test, predictions)))\n",
    "print('f1 score: ', format(f1_score(y_test, predictions)))\n",
    "print('recall score:', format(recall_score(y_test, predictions)))\n",
    "\n",
    "# Calculating Precision and Recall score on my own functions\n",
    "\"\"\"\n",
    "Seems the Score that I am getting is higher than that has been calcualted using metrics ??\n",
    "\"\"\"\n",
    "\n",
    "print('Precision, Recall', find_precision_recall_score(predictions, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
